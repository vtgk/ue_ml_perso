{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Using neural networks to predict and classify crystal structures of elements*\n",
    "\n",
    "**Why?** Neural networks are widely used for image classification, learning structures and substructures within the data to identify patterns. Such neural network based classifiers can identify patterns and correlations within the data.\n",
    "\n",
    "**What?** In this TP we will learn how to use Neural Networks to create a Classification Model to estimate the ground state of crystal structures. <br>\n",
    "\n",
    "**How to use this?** This TP uses python. Run each code cell in order by clicking \"Shift + Enter\". Feel free to modify the code, or change queries to familiarize yourself with the workings on the code.\n",
    "\n",
    "Suggested modifications and exercises are included in <font color=blue> blue</font>.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Getting a dataset\n",
    "2. Processing and Organizing Data\n",
    "3. Creating the Model\n",
    "4. Plotting\n",
    "\n",
    "\n",
    "**Get started:** Click \"Shift-Enter\" on the code cells to run! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting a dataset\n",
    "\n",
    "Datasets containing properties for the elements in the periodic table are available online; however, it would be thematic to create our own, using the tools from the first tutorial. In this section we will query both [Pymatgen](http://pymatgen.org/) and [Mendeleev](https://mendeleev.readthedocs.io/en/stable/) to get a complete set of properties per element. We will use this data to create the cases from which the model will train and test.\n",
    "<br>\n",
    "<br>\n",
    "In this first snippet of code we will import all relevant libraries, the elements that will be turned into cases and the properties that will serve as the attributes for the cases. We will get 47 entries (which is a small dataset), but should give us a somewhat accurate prediction. It is important to note that more entries would move the prediction closer to the real value, and so would more attributes.\n",
    "<br>\n",
    "<br>\n",
    "The elements listed were chosen because querying them for these properties yields a dataset with no unknown values, and because they represent the three most common crystallographic structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "import pymatgen as pymat\n",
    "import mendeleev as mendel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "fcc_elements = [\"Ag\", \"Al\", \"Au\", \"Cu\", \"Ir\", \"Ni\", \"Pb\", \"Pd\", \"Pt\", \"Rh\", \"Th\", \"Yb\"]\n",
    "bcc_elements = [\"Ba\", \"Ca\", \"Cr\", \"Cs\", \"Eu\", \"Fe\", \"Li\", \"Mn\", \"Mo\", \"Na\", \"Nb\", \"Rb\", \"Ta\", \"V\", \"W\" ]\n",
    "hcp_elements = [\"Be\", \"Cd\", \"Co\", \"Dy\", \"Er\", \"Gd\", \"Hf\", \"Ho\", \"Lu\", \"Mg\", \"Re\", \n",
    "                \"Ru\", \"Sc\", \"Tb\", \"Ti\", \"Tl\", \"Tm\", \"Y\", \"Zn\", \"Zr\"]\n",
    "\n",
    "elements = fcc_elements + bcc_elements + hcp_elements\n",
    "\n",
    "random.Random(1).shuffle(elements)\n",
    "\n",
    "querable_mendeleev = [\"atomic_number\", \"atomic_volume\", \"boiling_point\", \"en_ghosh\",  \"evaporation_heat\", \"heat_of_formation\",\n",
    "                     \"lattice_constant\", \"melting_point\", \"specific_heat\"]\n",
    "querable_pymatgen = [\"atomic_mass\", \"atomic_radius\", \"electrical_resistivity\",\"molar_volume\", \"bulk_modulus\", \"youngs_modulus\",\n",
    "                     \"average_ionic_radius\", \"density_of_solid\", \"coefficient_of_linear_thermal_expansion\"]\n",
    "querable_values = querable_mendeleev + querable_pymatgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will use the database queries to populate lists which can be displayed by the [Pandas](https://pandas.pydata.org/) library in a user-friendly table with the properties as the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = [] # Values for Attributes\n",
    "all_labels = [] # Crystal structure labels (0 = fcc, 1 = bcc, 2 = hcp)\n",
    "\n",
    "for item in elements:\n",
    "    element_values = []\n",
    "    \n",
    "    # This section queries Mendeleev\n",
    "    element_object = mendel.element(item)\n",
    "    for i in querable_mendeleev:    \n",
    "        element_values.append(getattr(element_object,i))\n",
    "\n",
    "    # This section queries Pymatgen\n",
    "    element_object = pymat.Element(item)    \n",
    "    for i in querable_pymatgen:\n",
    "        element_values.append(getattr(element_object,i))\n",
    "        \n",
    "    all_values.append(element_values) # All lists are appended to another list, creating a List of Lists\n",
    "    \n",
    "    if (item in fcc_elements):\n",
    "        all_labels.append([1, 0, 0]) # The crystal structure labels are assigned here\n",
    "    elif (item in bcc_elements):\n",
    "        all_labels.append([0, 1, 0]) # The crystal structure labels are assigned here\n",
    "    elif (item in hcp_elements):\n",
    "        all_labels.append([0, 0, 1]) # The crystal structure labels are assigned here\n",
    "\n",
    "# Pandas Dataframe\n",
    "df = pd.DataFrame(all_values, columns=querable_values)\n",
    "\n",
    "# We will patch some of the values that are not available in the datasets.\n",
    "\n",
    "# Value for the CTE of Cesium\n",
    "index_Cs = df.index[df['atomic_number'] == 55]\n",
    "df.iloc[index_Cs, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000097 \n",
    "# Value from: David R. Lide (ed), CRC Handbook of Chemistry and Physics, 84th Edition. CRC Press. Boca Raton, Florida, 2003\n",
    "\n",
    "# Value for the CTE of Rubidium\n",
    "index_Rb = df.index[df['atomic_number'] == 37]\n",
    "df.iloc[index_Rb, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000090 \n",
    "# Value from: https://www.azom.com/article.aspx?ArticleID=1834\n",
    "\n",
    "# Value for the Evaporation Heat of Ruthenium\n",
    "index_Ru = df.index[df['atomic_number'] == 44]\n",
    "df.iloc[index_Ru, df.columns.get_loc(\"evaporation_heat\")] = 595 # kJ/mol \n",
    "# Value from: https://www.webelements.com/ruthenium/thermochemistry.html\n",
    "\n",
    "# Value for the Bulk Modulus of Zirconium\n",
    "index_Zr = df.index[df['atomic_number'] == 40]\n",
    "df.iloc[index_Zr, df.columns.get_loc(\"bulk_modulus\")] = 94 # GPa \n",
    "# Value from: https://materialsproject.org/materials/mp-131/\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing and Organizing Data\n",
    "\n",
    "We again normalize the data and organize it into training and testing sets as before.\n",
    "\n",
    "##### SETS\n",
    "\n",
    "We have 47 elements for which the crystal structure is known and we will use 40 of these as a training set and the remaining 7 as testing set.\n",
    "\n",
    "##### NORMALIZATION\n",
    "\n",
    "We will again use the Standard Score Normalization, which subtracts the mean of the feature and divide by its standard deviation.\n",
    "\n",
    "<span style=\"font-size:2em;\">$ \\frac{X - µ}{σ} $ </span>\n",
    "\n",
    "While our model might converge without feature normalization, the resultant model would be difficult to train and would be dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SETS\n",
    "\n",
    "all_values = [list(df.iloc[x]) for x in range(len(all_values))]\n",
    "\n",
    "# List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "all_values = np.array(all_values, dtype = float)\n",
    "print(\"Shape of Values:\", all_values.shape)\n",
    "all_labels = np.array(all_labels, dtype = int)\n",
    "print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "# Training Set\n",
    "train_values = all_values[:40, :]\n",
    "train_labels = all_labels[:40, :]\n",
    "\n",
    "# Testing Set\n",
    "test_values = all_values[-7:, :]\n",
    "test_labels = all_labels[-7:, :]\n",
    "\n",
    "# NORMALIZATION\n",
    "\n",
    "mean = np.nanmean(train_values, axis = 0) # mean\n",
    "std = np.nanstd(train_values, axis = 0) # standard deviation\n",
    "\n",
    "train_values = (train_values - mean) / std # input scaling\n",
    "test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "print(train_values[0]) # print a sample entry from the training set\n",
    "#print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Model\n",
    "\n",
    "For this classification, we will use a simple sequential neural network with one densely connected hidden layer. The optimizer used will be [RMSprop](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop) (Root Mean Square Propagation).\n",
    "\n",
    "The key difference between the regression model and the classification model is our metric to measure network performance. While we used mean squared error (between the true outputs and the network's predicted output) for the regression task, we use categorical cross-entropy (click [here](https://en.wikipedia.org/wiki/Cross-entropy) to learn more about it), using classification accuracy as a metric where higher accuracy implies a better network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=14)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(train_values.shape[1],), kernel_initializer=kernel_init))\n",
    "#model.add(Dense(16, activation='relu', kernel_initializer=kernel_init))\n",
    "model.add(Dense(3, activation=tf.nn.softmax))  # Output Layer\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(0.002) # Root Mean Squared Propagation\n",
    "\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING \n",
    "\n",
    "This model is trained for 350 epochs, and we record the training accuracy in the history object. This way, by plotting \"history\" we can see the evolution of the \"learning\" of the model, that is the decrease of the Mean Absolute Error. Models in Keras are fitted to the training set using the [**fit**](https://keras.io/models/model/#fit) method.\n",
    "\n",
    "One **Epoch** occurs when you pass the entire dataset through the model. One **Batch** contains a subset of the dataset that can be fed to the model at the same time. As we have a really small dataset compared to the ones that are usually considered to be modeled by these neural networks, we are feeding all entries at the same time, so our batch is the entire dataset, and an epoch occurs when the batch is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + '\\r') # Updates current Epoch Number\n",
    "\n",
    "\n",
    "EPOCHS = 300 # Number of EPOCHS\n",
    "\n",
    "# HISTORY Object which contains how the model learned\n",
    "history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \\\n",
    "                    epochs=EPOCHS, validation_split=0.1, verbose = False, callbacks=[PrintEpNum()])\n",
    "\n",
    "# PLOTTING HISTORY USING MATPLOTLIB\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history.epoch, np.array(history.history['accuracy']),label='Training Accuracy')\n",
    "plt.plot(history.epoch, np.array(history.history['val_accuracy']),label = 'Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING\n",
    "\n",
    "Models in Keras are tested using the method [**evaluate**](https://keras.io/models/model/#evaluate). This method returns the classification accuracy on the training and the testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(train_values, train_labels, verbose=0)\n",
    "\n",
    "print(\"Training Set Accuracy: %f\" %(acc))\n",
    "\n",
    "loss, acc = model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing Set Accuracy: %f\" %(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAKING PREDICTIONS\n",
    "\n",
    "The last step in a Regression Model is to make predictions for values not in the training set, which are determined by the method [**predict**](https://keras.io/models/model/#predict). In the following cell we print the Elements in the testing set, the real values for their Young's Moduli and the predictions generated by the Machine Learning model.\n",
    "\n",
    "Finally, in the following cells, we will plot the predictions made by the model according to each Element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_values)\n",
    "test_predictions = model.predict(test_values)\n",
    "\n",
    "all_labels = np.vstack((train_labels, test_labels))\n",
    "all_predictions = np.vstack((train_predictions, test_predictions))\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    if (np.argmax(all_predictions[i]) == 0):\n",
    "        predicted_labels.append(\"FCC\")\n",
    "    if (np.argmax(all_labels[i]) == 0):\n",
    "        true_labels.append(\"FCC\")\n",
    "    if (np.argmax(all_predictions[i]) == 1):\n",
    "        predicted_labels.append(\"BCC\")\n",
    "    if (np.argmax(all_labels[i]) == 1):\n",
    "        true_labels.append(\"BCC\")\n",
    "    if (np.argmax(all_predictions[i]) == 2):\n",
    "        predicted_labels.append(\"HCP\")\n",
    "    if (np.argmax(all_labels[i]) == 2):\n",
    "        true_labels.append(\"HCP\")\n",
    "\n",
    "predicted_labels = np.array(predicted_labels).reshape((-1, 1))\n",
    "true_labels = np.array(true_labels).reshape((-1, 1))\n",
    "headings = [\"Atomic number\", \"True crystal structure\", \"Predicted crystal structure\"]\n",
    "\n",
    "atomic_number_array = np.array(df.iloc[:, 0]).reshape((-1, 1))\n",
    "plot_table = np.concatenate((atomic_number_array, true_labels, predicted_labels), axis=1)\n",
    "\n",
    "plot_df = pd.DataFrame(plot_table, columns=headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crystal_structures = [\"FCC\", \"BCC\", \"HCP\"]\n",
    "FCC_prediction = []\n",
    "BCC_prediction = []\n",
    "HCP_prediction = []\n",
    "\n",
    "for item in range(len(all_predictions)):\n",
    "    FCC_prediction.append(all_predictions[item].tolist()[0])\n",
    "    BCC_prediction.append(all_predictions[item].tolist()[1])\n",
    "    HCP_prediction.append(all_predictions[item].tolist()[2])   \n",
    "\n",
    "# --------------------------------------------------------------\n",
    "    \n",
    "# This block will be used to sort the elements by their atomic number\n",
    "    \n",
    "atomic_number = list(df.iloc[:, 0]) # From the Pandas Dataset\n",
    "order = np.argsort(atomic_number) # Sorting Indexes\n",
    "\n",
    "# Sorting the lists by the indexes\n",
    "# elements = [elements[x] for x in order]\n",
    "# FCC_prediction = [FCC_prediction[x] for x in order]\n",
    "# BCC_prediction = [BCC_prediction[x] for x in order]\n",
    "# HCP_prediction =[HCP_prediction[x] for x in order]\n",
    "\n",
    "# # --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot\n",
    "\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, vertical_spacing=0.2)\n",
    "\n",
    "# ---------\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in fcc_elements], y=[FCC_prediction[_] for _ in range(len(FCC_prediction)) if elements[_] in fcc_elements], name='FCC', marker=dict(color='green'), showlegend=False, textposition='inside', textfont={\"size\":24},\n",
    "                        text=['*'  if _ in elements[-7:] else None for _ in [_ for _ in elements if _ in fcc_elements]]), row=1, col=1)\n",
    "\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in fcc_elements], y=[BCC_prediction[_] for _ in range(len(BCC_prediction)) if elements[_] in fcc_elements], name='BCC', marker=dict(color='red'), showlegend=False), row=1, col=1)\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in fcc_elements], y=[HCP_prediction[_] for _ in range(len(HCP_prediction)) if elements[_] in fcc_elements], name='HCP', marker=dict(color='red'), showlegend=False), row=1, col=1) \n",
    "# ---------\n",
    "\n",
    "# ---------\n",
    "\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in bcc_elements], y=[FCC_prediction[_] for _ in range(len(FCC_prediction)) if elements[_] in bcc_elements], name='FCC', marker=dict(color='red'), showlegend=False), row=2, col=1)\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in bcc_elements], y=[BCC_prediction[_] for _ in range(len(BCC_prediction)) if elements[_] in bcc_elements], name='BCC', marker=dict(color='green'), showlegend=False, textposition='outside', textfont={\"size\":24},\n",
    "                        text=['*'  if _ in elements[-7:] else None for _ in [_ for _ in elements if _ in bcc_elements]]), row=2, col=1)\n",
    "\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in bcc_elements], y=[HCP_prediction[_] for _ in range(len(HCP_prediction)) if elements[_] in bcc_elements], name='HCP', marker=dict(color='red'), showlegend=False), row=2, col=1)\n",
    "# ---------\n",
    "\n",
    "# ---------\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in hcp_elements], y=[FCC_prediction[_] for _ in range(len(FCC_prediction)) if elements[_] in hcp_elements], name='FCC', marker=dict(color='red'), showlegend=False), row=3, col=1)\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in hcp_elements], y=[BCC_prediction[_] for _ in range(len(BCC_prediction)) if elements[_] in hcp_elements], name='BCC', marker=dict(color='red'), showlegend=False), row=3, col=1)\n",
    "fig.append_trace(go.Bar(x=[_ for _ in elements if _ in hcp_elements], y=[HCP_prediction[_] for _ in range(len(HCP_prediction)) if elements[_] in hcp_elements], name='HCP', marker=dict(color='green'), showlegend=False, textposition='inside', textfont={\"size\":24},\n",
    "                        text=['*'  if _ in elements[-7:] else None for _ in [_ for _ in elements if _ in hcp_elements]]), row=3, col=1)\n",
    "# ---------\n",
    "\n",
    "\n",
    "fig.update_xaxes(title=go.layout.xaxis.Title(text=\"FCC Elements\", font=dict(size=18)),showgrid=True, tickfont=dict(size=18), row=1, col=1)\n",
    "fig.update_xaxes(title=go.layout.xaxis.Title(text=\"BCC Elements\", font=dict(size=18)),showgrid=True, tickfont=dict(size=18), row=2, col=1)\n",
    "fig.update_xaxes(title=go.layout.xaxis.Title(text=\"HCP Elements\", font=dict(size=18)),showgrid=True, tickfont=dict(size=18), row=3, col=1)\n",
    "\n",
    "fig.update_yaxes(title=go.layout.yaxis.Title(text=\"Probability\", font=dict(size=18)),showgrid=True, tickfont=dict(size=18),range=[0, 1.2], row=1, col=1)\n",
    "fig.update_yaxes(title=go.layout.yaxis.Title(text=\"Probability\", font=dict(size=18)),showgrid=True, tickfont=dict(size=18),range=[0, 1.2], row=2, col=1)\n",
    "fig.update_yaxes(title=go.layout.yaxis.Title(text=\"Probability\", font=dict(size=18)),showgrid=True, tickfont=dict(size=18),range=[0, 1.2], row=3, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(height=700, width=1200, barmode='group', bargap=0.3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * <font color=blue> **Exercise 1.** The model's performances could be sensitive to the choice of training set. For this reason, one typically performs [K-fold cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) to estimate the reliability of the model depending on the dataset. Implement a K-fold cross-validation strategy (using sk-learn for instance), and report the performances (the mean cross-validation accuracy, and the standard deviation of cross-validation accuracy). Comment these performances.  </font>\n",
    " <br>\n",
    " \n",
    "  * <font color=blue> **Exercise 2.** Evaluate how the model performances vary with the dataset size. Although we are limited in terms of available data, construct several models with variable number of data points, and plot its performances as a function of dataset size.  </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
